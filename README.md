```markdown
# ğŸ§¾ AWS Real-Time Order History Data Pipeline

This project demonstrates a real-time data ingestion and transformation pipeline using AWS services like **EC2**, **Kinesis Firehose**, **S3**, **Glue**, **Athena**, and **QuickSight**. The pipeline processes e-commerce order logs, enriches the data, and stores it for analytical queries and visualization.

---

## ğŸ“Š Architecture

```

EC2 (Log Generator)
â†“
AWS Kinesis Firehose (tbsm-red-fire)
â†“
Amazon S3 (Raw Logs Bucket)
â†“
AWS Glue Crawler â†’ AWS Glue Job (Data Transformations)
â†“
Amazon S3 (Partitioned Output)
â†“
Amazon Athena / Amazon QuickSight

```

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ data-generator/
â”‚   â””â”€â”€ LogGenerator.py            # Simulates and writes order logs to /var/log/orders/
â”‚
â”œâ”€â”€ kinesis-agent/
â”‚   â””â”€â”€ agent.json                 # Kinesis Agent config to push logs to Firehose
â”‚
â”œâ”€â”€ glue-job/
â”‚   â””â”€â”€ transform\_order\_history.py # Main Glue ETL job script with all transformations
â”‚
â”œâ”€â”€ athena/
â”‚   â”œâ”€â”€ create\_table.sql           # Athena external table over transformed Parquet files
â”‚   â””â”€â”€ queries.sql                # Sample Athena queries for insights
â”‚
â””â”€â”€ README.md

````

---

## âš™ï¸ Setup Instructions

### 1. ğŸš€ Real-Time Log Generation

- Launch EC2 instance with the necessary permissions (Firehose, S3).
- Place `LogGenerator.py` in the EC2 and execute it to start writing log files to `/var/log/orders/`.

```bash
python3 LogGenerator.py
````

---

### 2. ğŸ”¥ Kinesis Firehose

* Create a Firehose delivery stream named `tbsm-red-fire` pointing to your raw S3 bucket.
* On EC2, install and configure the **Kinesis Agent**:

```bash
sudo yum install -y aws-kinesis-agent
sudo cp kinesis-agent/agent.json /etc/aws-kinesis/agent.json
sudo service aws-kinesis-agent start
```

---

### 3. ğŸ§¬ AWS Glue

* Create a **Glue Crawler** to scan raw S3 logs.
* Create a **Glue Job** using `glue-job/transform_order_history.py` that:

  * Generates unique Order IDs
  * Converts to USD
  * Splits Date/Time
  * Maps Country â†’ Region
  * Tags high-value orders
  * Filters bad records
  * Writes to S3 (Parquet, partitioned by Country & Date)

---

### 4. ğŸ§  Amazon Athena

* Run `athena/create_table.sql` to register your transformed data as an external table.
* Execute queries from `athena/queries.sql` to explore insights like:

  * Top 10 high value orders
  * Region-wise orders
  * Peak ordering hours
  * Query by country partitions

---

### 5. ğŸ“ˆ Amazon QuickSight

* Connect QuickSight to the S3/Athena dataset.
* Build visualizations for:

  * Daily order trends
  * Region-wise heatmaps
  * Order volume by hour

---

## âœ… Transformations Performed

| Transformation        | Description                                                              |
| --------------------- | ------------------------------------------------------------------------ |
| ğŸ”‘ OrderID            | SHA-256 Hash of `InvoiceNo + StockCode`                                  |
| ğŸ’° TotalAmountUSD     | `Quantity * UnitPrice * ConversionRate (0.012)`                          |
| ğŸ•’ InvoiceDate Split  | Parsed into `InvoiceDateOnly` and `InvoiceHour`                          |
| ğŸŒ Region Mapping     | Maps Country â†’ Region                                                    |
| ğŸ§¹ Data Cleaning      | Filters rows with missing `CustomerID`, or negative `Quantity/UnitPrice` |
| ğŸ·ï¸ Order Tag         | Marks orders with value > 100 as `High`                                  |
| ğŸ§± Partitioned Output | Written to S3 by `Country` and `InvoiceDateOnly` in **Parquet** format   |

---

## ğŸ§  Sample Use Cases

* ğŸ” Business analysis of high-value customers
* ğŸ—ºï¸ Geo insights: orders per region/country
* â° Time-based patterns: hourly peaks
* ğŸ“¦ Detect and clean bad input data

```
